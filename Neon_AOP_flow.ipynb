{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2389b62-0b84-4c4b-ad24-087e9c3d53b4",
   "metadata": {},
   "source": [
    "## Setup Flow with Prefect Cloud\n",
    "\n",
    "**Only need to do this one time**\n",
    "\n",
    "Set backend to cloud (unless running prefect server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bd4e5-802b-43fb-8e36-8f5a0c5c83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefect backend cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df3aa1-71df-43d3-8024-60e687f1095d",
   "metadata": {},
   "source": [
    "Authentification - https://docs.prefect.io/orchestration/tutorial/overview.html#create-an-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41225647-f20c-40c5-8dfe-48c8a4707c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "prompts = 'Enter your Prefect Cloud API key: '\n",
    "apikey = getpass(prompt='Enter your Prefect Cloud API key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc66b14-9d30-4571-b6ec-70a88091286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ~/.prefect/config.toml\n",
    "!echo -en \"[cloud]\\nauth_token = \\\"$apikey\\\"\" >> ~/.prefect/config.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f50847-b3fe-4e4f-82e5-42764ad8f8b9",
   "metadata": {},
   "source": [
    "Create a project on Prefect Cloud - https://docs.prefect.io/orchestration/tutorial/first.html#creating-a-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00609d-715d-4155-970c-4583e2e5e5e2",
   "metadata": {},
   "source": [
    "## Create Dask Execution Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a91f1-c015-4876-b2d2-f5d6a0f410e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask, dask.distributed\n",
    "dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf08306-c48e-4904-8a81-b314526d323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_type = 'local'\n",
    "#cluster_type = 'HPC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949081f-bff4-4126-b212-07d4edb0f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'local':\n",
    "    dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "    cluster = LocalCluster(n_workers=6)#,threads_per_worker=2)\n",
    "    cl = Client(cluster)\n",
    "elif cluster_type == 'HPC':\n",
    "    import dask_jobqueue as jq\n",
    "    dask.config.set({'distributed.dashboard.link': '/user/{user}/proxy/{port}/status'})\n",
    "    partition='brief-low'#,debug,mem,mem-low'\n",
    "    num_processes = 10\n",
    "    num_threads_per_processes = 4\n",
    "    mem = 3.2*num_processes*num_threads_per_processes#*1.25\n",
    "    n_cores_per_job = num_processes*num_threads_per_processes\n",
    "    container = 'docker://rowangaffney/data_science_im_rs:latest'\n",
    "    env = 'py_geo'\n",
    "    clust = jq.SLURMCluster(queue=partition,\n",
    "                            processes=num_processes,\n",
    "                            cores=n_cores_per_job,\n",
    "                            memory=str(mem)+'GB',\n",
    "                            interface='ib0',\n",
    "                            local_directory='$TMPDIR',\n",
    "                            death_timeout=30,\n",
    "                            python=\"singularity -vv exec {} /opt/conda/envs/{}/bin/python\".format(container,env),\n",
    "                            walltime='02:00:00',\n",
    "                            job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "    cl=Client(clust)\n",
    "    dash_addr = '''/user/{}/proxy/{}/status'''.format(os.environ['USER'],cl.scheduler_info()['services']['dashboard'])\n",
    "    print('Dask Lab Extention Address (paste into the search box): '+dash_addr)\n",
    "    \n",
    "    #Scale Cluster \n",
    "    num_jobs=8\n",
    "    clust.scale(n=num_jobs*num_processes)\n",
    "else:\n",
    "    print('Cluster type not defined')\n",
    "ncpus = int(np.unique(np.array(list(cl.nthreads().values())))[0])\n",
    "cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158e83c-c005-49a9-b802-01f2ec1bbb01",
   "metadata": {},
   "source": [
    "## Build Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b6fc0-99f5-4e29-ba89-17c4fabb3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Prefect Packages\n",
    "from prefect import task, Flow, unmapped, Parameter\n",
    "from prefect.executors import DaskExecutor\n",
    "import prefect\n",
    "\n",
    "#Import custom tasks\n",
    "from aop_mosaic import tasks as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596f770-ffac-47fa-a93e-bd96a396ed48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fdb9a-4866-4779-bc32-de600a4d9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(name = 'Mosaic_NEON_AOP_Hyper',\n",
    "          executor=DaskExecutor(address=cl.scheduler_info()['address'])\n",
    "         ) as flow:\n",
    "    \n",
    "    #### Define Parameters ###\n",
    "    site_p = Parameter('site_p', default = 'CPER')\n",
    "    processDate_p = Parameter('processDate_p', default = '2017-05')\n",
    "    result_folder = Parameter('res_folder', default = './')\n",
    "    \n",
    "    \n",
    "    #Task1: Get metadata about the data product for a specific site / date\n",
    "    site_dict = t.query_data_urls(site=site_p,\n",
    "                                processDate=processDate_p)\n",
    "    \n",
    "    #Task2: Get the URLS for all the h5 files.\n",
    "    h5_files = t.query_file_urls(site_dict=site_dict,\n",
    "                               site=site_p,\n",
    "                               processDate=processDate_p)\n",
    "    \n",
    "    #Task3: Setup the BRDF and TOPO correction configurations\n",
    "    workflow_meta = t.BRDF_TOPO_Config.map(pipeline_dict = h5_files,\n",
    "                                        site=unmapped(site_p),\n",
    "                                        processDate=unmapped(processDate_p),\n",
    "                                        cpus=unmapped(ncpus))\n",
    "    \n",
    "    #Task4: Download the files to folder ./{site}_{procossDate}/\n",
    "    download_res = t.download_file.map(pipeline_dict = workflow_meta,\n",
    "                                 site=unmapped(site_p),\n",
    "                                 processDate=unmapped(processDate_p))\n",
    "    \n",
    "    #Task5: Get the metadata for each file\n",
    "    workflow_meta = t.get_file_meta.map(pipeline_dict=download_res)\n",
    "    \n",
    "    #Task6: Write the metadata for each file to a human readable file (.json)\n",
    "    metadata_exported = t.write_pipeline_meta(pipeline_dict = workflow_meta,\n",
    "                                            site=unmapped(site_p),\n",
    "                                            processDate=unmapped(processDate_p))\n",
    "    \n",
    "    #Task7: Apply the BRDF and TOPO corrections to the data\n",
    "    ht_pipeline = t.apply_corrections_mosaic.map(pipeline_dict=workflow_meta,\n",
    "                                               site=unmapped(site_p),\n",
    "                                               processDate=unmapped(processDate_p))\n",
    "    \n",
    "    #Task8: Get the mask for each flight line for mosaicing the flights together\n",
    "    ht_pipeline2 = t.pixel_mosaic_mask.map(pipeline_dict = ht_pipeline,\n",
    "                                        pipeline_list=unmapped(ht_pipeline))\n",
    "    \n",
    "    #Task9: Get the extents of all the flights for the final mosaic.\n",
    "    extent = t.moasic_extent(ht_pipeline2)\n",
    "    \n",
    "    #Task10: Mosaic the BRDF and Topo corrected flights using pixels to the lowest sensor to-zenith angle\n",
    "    success = t.mosaic(pipeline_list=ht_pipeline,\n",
    "                       extents=extent,\n",
    "                       site=site_p,\n",
    "                       processDate=processDate_p,\n",
    "                      result_folder = res_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c970e-d8bc-4075-b6f0-31e833cd0518",
   "metadata": {},
   "source": [
    "## Register FLow and Start of Local Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5567d8b-cb64-483f-bc3d-a42b795cba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the flow under the \"tutorial\" project\n",
    "flow.register(project_name=\"Neon_AOP_BRDF_Mosaic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eabca2-e23e-45e1-84a0-5731ca1d99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefect agent local start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bdace-5f60-42de-846c-bc9a46fba524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py_geo]",
   "language": "python",
   "name": "conda-env-py_geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
